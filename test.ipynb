{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=5\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from trl import (SFTTrainer, SFTConfig)\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, TrainingArguments)\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"huggingface/meta-llama/Llama-3.2-3B-Instruct\"\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "tokenizer : PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67447da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_id = \"huggingface/allenai/tulu-3-sft-mixture\"\n",
    "\n",
    "raw_ds = load_dataset(datasets_id, split=\"train\")\n",
    "\n",
    "def filter_func(x):\n",
    "    msg = x[\"messages\"]\n",
    "    if not any(m[\"role\"] == \"assistant\" and m[\"content\"].strip() != \"\" for m in msg):\n",
    "        return False\n",
    "    src = x[\"source\"]\n",
    "    allowed_src = [\"math\", \"science\", \"history\", \"literature\"]\n",
    "    for allowed in allowed_src:\n",
    "        if allowed in src:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_ds = raw_ds.filter(filter_func).flatten_indices()\n",
    "\n",
    "raw_ds = raw_ds.train_test_split(test_size=0.005, seed=42)\n",
    "\n",
    "train_ds = raw_ds[\"train\"]\n",
    "eval_ds = raw_ds[\"test\"]\n",
    "\n",
    "train_ds = train_ds.shuffle(seed=42)\n",
    "\n",
    "mini_ds = train_ds.select(range(10))\n",
    "\n",
    "print(\"size of train dataset: \", len(train_ds))\n",
    "print(\"size of eval dataset: \", len(eval_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_template = \"\"\n",
    "\n",
    "with open(\"llama-3.2.jinja2\", \"r\", encoding=\"utf-8\") as f:\n",
    "    my_template = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer \n",
    "\n",
    "msg = mini_ds[0][\"messages\"]\n",
    "\n",
    "processed = tokenizer.apply_chat_template(msg, return_dict=True, return_assistant_tokens_mask=True, chat_template=my_template)\n",
    "\n",
    "print(processed[\"assistant_masks\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_str = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(len(origin_str))\n",
    "\n",
    "my_str = tokenizer.apply_chat_template(msg, tokenize=False, chat_template=my_template, add_generation_prompt=True)\n",
    "\n",
    "print(len(my_str))\n",
    "\n",
    "print(origin_str == my_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf6b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(processed[\"input_ids\"])\n",
    "assistant_masks = torch.tensor(processed[\"assistant_masks\"]).to(torch.bool)\n",
    "\n",
    "\n",
    "# print(input_ids)\n",
    "# print(assistant_masks)\n",
    "\n",
    "gen_ids = input_ids[assistant_masks]\n",
    "\n",
    "tokenizer.decode(gen_ids[-1:])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_args = SFTConfig(\n",
    "    output_dir=\"./output/test\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    assistant_only_loss=True,\n",
    "    # dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=mini_ds,\n",
    "    eval_dataset=mini_ds,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788faaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# 6) 保存（仅保存 LoRA adapter 权重）\n",
    "from peft import PeftModel\n",
    "if isinstance(model, PeftModel):\n",
    "    model.save_pretrained(\"./out-lora-tulu3/adapter\")\n",
    "else:\n",
    "    # 意外情况（例如未套 PEFT）：存整模型\n",
    "    model.save_pretrained(\"./out-lora-tulu3/full\")\n",
    "tokenizer.save_pretrained(\"./out-lora-tulu3\")\n",
    "print(\"✅ Done. Saved to ./out-lora-tulu3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"huggingface/allenai/tulu-3-sft-mixture\")\n",
    "\n",
    "# ds.save_to_disk(Path(\"huggingface\") / \"allenai/tulu-3-sft-mixture\" / \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a0a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
