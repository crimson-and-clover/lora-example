{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from trl import (SFTTrainer, SFTConfig)\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM)\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = Path(\"./datasets/tulu-math\")\n",
    "\n",
    "raw_ds = load_from_disk(datasets_path)\n",
    "\n",
    "train_size = 10000\n",
    "test_size = 100\n",
    "eval_size = 10\n",
    "\n",
    "raw_ds = raw_ds.train_test_split(\n",
    "    train_size=train_size, test_size=test_size + eval_size, seed=42)\n",
    "\n",
    "train_ds = raw_ds[\"train\"]\n",
    "test_ds = raw_ds[\"test\"]\n",
    "eval_ds = test_ds.select(range(eval_size))\n",
    "test_ds = test_ds.select(range(eval_size, eval_size + test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "\n",
    "# model_name = \"huggingface/meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_name = \"huggingface/Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, dtype=dtype, device_map=device)\n",
    "base_model = base_model.eval()\n",
    "\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "for start in tqdm(range(0, len(eval_ds), batch_size)):\n",
    "    end = min(start + batch_size, len(eval_ds))\n",
    "    gt_msgs = [eval_ds[i][\"messages\"] for i in range(start, end)]\n",
    "    batch_msgs = [eval_ds[i][\"messages\"][:-1] for i in range(start, end)]\n",
    "    batch_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            msg, tokenize=False, add_generation_prompt=True)\n",
    "        for msg in batch_msgs\n",
    "    ]\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    inputs = tokenizer.apply_chat_template(batch_msgs, tokenize=True,\n",
    "                                           return_dict=True,\n",
    "                                           add_generation_prompt=True,\n",
    "                                           padding=True,\n",
    "                                           return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=dtype):\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=4096,\n",
    "            )\n",
    "\n",
    "    gen_ids = outputs[:, inputs.input_ids.shape[1]:]\n",
    "    gen_strs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "    gen_msgs = [\n",
    "        eval_ds[i][\"messages\"][:-1] +\n",
    "        [{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": gen_strs[i]\n",
    "        }]\n",
    "        for i in range(start, end)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "metric_bleu = load(\"bleu\")\n",
    "metric_rouge = load(\"rouge\")\n",
    "metric_bertscore = load(\"bertscore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62018b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_result = metric_bleu.compute(\n",
    "    predictions=[gen_msgs[0][-1][\"content\"]],\n",
    "    references=[gen_msgs[0][-2][\"content\"]]\n",
    ")\n",
    "\n",
    "print(bleu_result)\n",
    "\n",
    "rouge_result = metric_rouge.compute(\n",
    "    predictions=[gen_msgs[0][-1][\"content\"]],\n",
    "    references=[gen_msgs[0][-2][\"content\"]],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(rouge_result)\n",
    "\n",
    "bertscore_result = metric_bertscore.compute(\n",
    "    predictions=[gen_msgs[0][-1][\"content\"]],\n",
    "    references=[gen_msgs[0][-2][\"content\"]],\n",
    "    lang=\"en\",                # 指定语言\n",
    "    model_type=\"roberta-large\",  # 用哪个模型做嵌入\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(bertscore_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./output/qwen3_4b_instruct/tulu_math\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "batch_size = 10\n",
    "model = base_model.eval().to(\"cuda\")\n",
    "\n",
    "for start in tqdm(range(0, len(eval_ds), batch_size)):\n",
    "    end = min(start + batch_size, len(eval_ds))\n",
    "    batch_msgs = [eval_ds[i][\"messages\"][:-1] for i in range(start, end)]\n",
    "\n",
    "    # 生成批量 prompt 文本\n",
    "    batch_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            msg, tokenize=False, add_generation_prompt=True)\n",
    "        for msg in batch_msgs\n",
    "    ]\n",
    "\n",
    "    # 批量 tokenize\n",
    "    inputs = tokenizer(\n",
    "        batch_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=5000,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # 解码结果\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # 保存每条生成\n",
    "    for i, text in enumerate(decoded):\n",
    "        idx = start + i\n",
    "        (output_dir / f\"{idx:04d}.md\").write_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d944968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\",\n",
    "                    \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67447da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_id = \"huggingface/allenai/tulu-3-sft-mixture\"\n",
    "\n",
    "raw_ds = load_dataset(datasets_id, split=\"train\")\n",
    "\n",
    "\n",
    "def filter_func(x):\n",
    "    msg = x[\"messages\"]\n",
    "    if not any(m[\"role\"] == \"assistant\" and m[\"content\"].strip() != \"\" for m in msg):\n",
    "        return False\n",
    "    src = x[\"source\"]\n",
    "    allowed_src = [\"math\", \"science\", \"history\", \"literature\"]\n",
    "    for allowed in allowed_src:\n",
    "        if allowed in src:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_ds = raw_ds.filter(filter_func).flatten_indices()\n",
    "\n",
    "raw_ds.save_to_disk(\"./datasets/tulu-math\")\n",
    "\n",
    "# raw_ds = raw_ds.train_test_split(test_size=0.005, seed=42)\n",
    "\n",
    "# train_ds = raw_ds[\"train\"]\n",
    "# eval_ds = raw_ds[\"test\"]\n",
    "\n",
    "# train_ds = train_ds.shuffle(seed=42)\n",
    "\n",
    "# mini_ds = train_ds.select(range(10))\n",
    "\n",
    "# print(\"size of train dataset: \", len(train_ds))\n",
    "# print(\"size of eval dataset: \", len(eval_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_template = \"\"\n",
    "\n",
    "with open(\"llama-3.2.jinja2\", \"r\", encoding=\"utf-8\") as f:\n",
    "    my_template = f.read()\n",
    "\n",
    "tokenizer.chat_template = my_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_args = SFTConfig(\n",
    "    output_dir=\"./output/test\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    assistant_only_loss=True,\n",
    "    # dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788faaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "if isinstance(model, PeftModel):\n",
    "    model.save_pretrained(\"./output/test/adapter\")\n",
    "else:\n",
    "    # 意外情况（例如未套 PEFT）：存整模型\n",
    "    model.save_pretrained(\"./output/test/full\")\n",
    "tokenizer.save_pretrained(\"./output/test\")\n",
    "print(\"✅ Done. Saved to ./output/test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
